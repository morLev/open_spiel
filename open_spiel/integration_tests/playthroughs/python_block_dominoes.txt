game: python_block_dominoes

GameType.chance_mode = ChanceMode.EXPLICIT_STOCHASTIC
GameType.dynamics = Dynamics.SEQUENTIAL
GameType.information = Information.IMPERFECT_INFORMATION
GameType.long_name = "Python block dominoes"
GameType.max_num_players = 2
GameType.min_num_players = 2
GameType.parameter_specification = []
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = True
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = True
GameType.reward_model = RewardModel.TERMINAL
GameType.short_name = "python_block_dominoes"
GameType.utility = Utility.ZERO_SUM

NumDistinctActions() = 154
PolicyTensorShape() = [154]
MaxChanceOutcomes() = 28
GetParameters() = {}
NumPlayers() = 2
MinUtility() = -69.0
MaxUtility() = 69.0
UtilitySum() = 0.0
InformationStateTensorShape() = player: [2], count_unseen_pips: [7], hand: [7, 3], hand_sizes: [2], actions: [7, 5], edges: [3]
InformationStateTensorLayout() = TensorLayout.CHW
InformationStateTensorSize() = 70
ObservationTensorShape() = player: [2], count_unseen_pips: [7], hand: [7, 3], hand_sizes: [2], actions: [7, 5], edges: [3]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 70
MaxGameLength() = 28
ToString() = "python_block_dominoes()"

# State 0
# hand0:[] hand1:[] history:[]
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.CHANCE
InformationStateString(0) = "p0 hand:[]"
InformationStateString(1) = "p1 hand:[]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).count_unseen_pips = [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
InformationStateTensor(0).hand: ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
InformationStateTensor(0).hand_sizes: ◯◯
InformationStateTensor(0).actions: ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
InformationStateTensor(0).edges: ◯◯◯
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).count_unseen_pips = [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
InformationStateTensor(1).hand: ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
InformationStateTensor(1).hand_sizes: ◯◯
InformationStateTensor(1).actions: ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
InformationStateTensor(1).edges: ◯◯◯
ObservationString(0) = "p0 hand:[]"
ObservationString(1) = "p1 hand:[]"
PublicObservationString() = "p0 hand:[]"
PrivateObservationString(0) = "p0 hand:[]"
PrivateObservationString(1) = "p1 hand:[]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).count_unseen_pips = [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
ObservationTensor(0).hand: ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
ObservationTensor(0).hand_sizes: ◯◯
ObservationTensor(0).actions: ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
ObservationTensor(0).edges: ◯◯◯
ObservationTensor(1).player: ◯◉
ObservationTensor(1).count_unseen_pips = [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
ObservationTensor(1).hand: ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
ObservationTensor(1).hand_sizes: ◯◯
ObservationTensor(1).actions: ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
ObservationTensor(1).edges: ◯◯◯
ChanceOutcomes() = [(0, 0.03571428571428571), (1, 0.03571428571428571), (2, 0.03571428571428571), (3, 0.03571428571428571), (4, 0.03571428571428571), (5, 0.03571428571428571), (6, 0.03571428571428571), (7, 0.03571428571428571), (8, 0.03571428571428571), (9, 0.03571428571428571), (10, 0.03571428571428571), (11, 0.03571428571428571), (12, 0.03571428571428571), (13, 0.03571428571428571), (14, 0.03571428571428571), (15, 0.03571428571428571), (16, 0.03571428571428571), (17, 0.03571428571428571), (18, 0.03571428571428571), (19, 0.03571428571428571), (20, 0.03571428571428571), (21, 0.03571428571428571), (22, 0.03571428571428571), (23, 0.03571428571428571), (24, 0.03571428571428571), (25, 0.03571428571428571), (26, 0.03571428571428571), (27, 0.03571428571428571)]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]
StringLegalActions() = ["Deal (6.0, 6.0)", "Deal (5.0, 5.0)", "Deal (4.0, 4.0)", "Deal (3.0, 3.0)", "Deal (2.0, 2.0)", "Deal (1.0, 1.0)", "Deal (0.0, 0.0)", "Deal (5.0, 6.0)", "Deal (4.0, 6.0)", "Deal (3.0, 6.0)", "Deal (4.0, 5.0)", "Deal (2.0, 6.0)", "Deal (3.0, 5.0)", "Deal (1.0, 6.0)", "Deal (2.0, 5.0)", "Deal (3.0, 4.0)", "Deal (0.0, 6.0)", "Deal (1.0, 5.0)", "Deal (2.0, 4.0)", "Deal (0.0, 5.0)", "Deal (1.0, 4.0)", "Deal (2.0, 3.0)", "Deal (0.0, 4.0)", "Deal (1.0, 3.0)", "Deal (0.0, 3.0)", "Deal (1.0, 2.0)", "Deal (0.0, 2.0)", "Deal (0.0, 1.0)"]

# Apply action "Deal (0.0, 5.0)"
action: 19

# State 1
# hand0:['(0.0, 5.0)'] hand1:[] history:[]
IsTerminal() = False
History() = [19]
HistoryString() = "19"
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.CHANCE
InformationStateString(0) = "p0 hand:[(0.0, 5.0)]"
InformationStateString(1) = "p1 hand:[]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).count_unseen_pips = [6.0, 7.0, 7.0, 7.0, 7.0, 6.0, 7.0]
InformationStateTensor(0).hand = [0.0, 0.83333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).hand_sizes = [0.14286, 0.0]
InformationStateTensor(0).actions = [0.0, 0.83333, 0.0, 0.83333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).edges: ◯◯◯
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).count_unseen_pips = [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
InformationStateTensor(1).hand: ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
InformationStateTensor(1).hand_sizes = [0.0, 0.14286]
InformationStateTensor(1).actions: ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
InformationStateTensor(1).edges: ◯◯◯
ObservationString(0) = "p0 hand:[(0.0, 5.0)]"
ObservationString(1) = "p1 hand:[]"
PublicObservationString() = "p0 hand:[(0.0, 5.0)]"
PrivateObservationString(0) = "p0 hand:[(0.0, 5.0)]"
PrivateObservationString(1) = "p1 hand:[]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).count_unseen_pips = [6.0, 7.0, 7.0, 7.0, 7.0, 6.0, 7.0]
ObservationTensor(0).hand = [0.0, 0.83333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).hand_sizes = [0.14286, 0.0]
ObservationTensor(0).actions = [0.0, 0.83333, 0.0, 0.83333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).edges: ◯◯◯
ObservationTensor(1).player: ◯◉
ObservationTensor(1).count_unseen_pips = [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]
ObservationTensor(1).hand: ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
ObservationTensor(1).hand_sizes = [0.0, 0.14286]
ObservationTensor(1).actions: ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
ObservationTensor(1).edges: ◯◯◯
ChanceOutcomes() = [(0, 0.037037037037037035), (1, 0.037037037037037035), (2, 0.037037037037037035), (3, 0.037037037037037035), (4, 0.037037037037037035), (5, 0.037037037037037035), (6, 0.037037037037037035), (7, 0.037037037037037035), (8, 0.037037037037037035), (9, 0.037037037037037035), (10, 0.037037037037037035), (11, 0.037037037037037035), (12, 0.037037037037037035), (13, 0.037037037037037035), (14, 0.037037037037037035), (15, 0.037037037037037035), (16, 0.037037037037037035), (17, 0.037037037037037035), (18, 0.037037037037037035), (20, 0.037037037037037035), (21, 0.037037037037037035), (22, 0.037037037037037035), (23, 0.037037037037037035), (24, 0.037037037037037035), (25, 0.037037037037037035), (26, 0.037037037037037035), (27, 0.037037037037037035)]
LegalActions() = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27]
StringLegalActions() = ["Deal (6.0, 6.0)", "Deal (5.0, 5.0)", "Deal (4.0, 4.0)", "Deal (3.0, 3.0)", "Deal (2.0, 2.0)", "Deal (1.0, 1.0)", "Deal (0.0, 0.0)", "Deal (5.0, 6.0)", "Deal (4.0, 6.0)", "Deal (3.0, 6.0)", "Deal (4.0, 5.0)", "Deal (2.0, 6.0)", "Deal (3.0, 5.0)", "Deal (1.0, 6.0)", "Deal (2.0, 5.0)", "Deal (3.0, 4.0)", "Deal (0.0, 6.0)", "Deal (1.0, 5.0)", "Deal (2.0, 4.0)", "Deal (1.0, 4.0)", "Deal (2.0, 3.0)", "Deal (0.0, 4.0)", "Deal (1.0, 3.0)", "Deal (0.0, 3.0)", "Deal (1.0, 2.0)", "Deal (0.0, 2.0)", "Deal (0.0, 1.0)"]

# Apply action "Deal (0.0, 4.0)"
action: 22

# State 2
# Apply action "Deal (4.0, 5.0)"
action: 10

# State 3
# Apply action "Deal (1.0, 5.0)"
action: 17

# State 4
# Apply action "Deal (3.0, 4.0)"
action: 15

# State 5
# Apply action "Deal (0.0, 3.0)"
action: 24

# State 6
# Apply action "Deal (2.0, 2.0)"
action: 4

# State 7
# Apply action "Deal (1.0, 4.0)"
action: 20

# State 8
# Apply action "Deal (2.0, 5.0)"
action: 14

# State 9
# Apply action "Deal (6.0, 6.0)"
action: 0

# State 10
# Apply action "Deal (1.0, 1.0)"
action: 5

# State 11
# Apply action "Deal (0.0, 6.0)"
action: 16

# State 12
# Apply action "Deal (4.0, 6.0)"
action: 8

# State 13
# Apply action "Deal (4.0, 4.0)"
action: 2

# State 14
# hand0:['(0.0, 6.0)', '(1.0, 1.0)', '(1.0, 4.0)', '(2.0, 5.0)', '(4.0, 4.0)', '(4.0, 6.0)', '(6.0, 6.0)'] hand1:['(0.0, 3.0)', '(0.0, 4.0)', '(0.0, 5.0)', '(1.0, 5.0)', '(2.0, 2.0)', '(3.0, 4.0)', '(4.0, 5.0)'] history:[]
IsTerminal() = False
History() = [19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2]
HistoryString() = "19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "p0 hand:[(0.0, 6.0), (1.0, 1.0), (1.0, 4.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
InformationStateString(1) = "p1 hand:[(0.0, 3.0), (0.0, 4.0), (0.0, 5.0), (1.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).count_unseen_pips = [6.0, 5.0, 6.0, 7.0, 4.0, 6.0, 4.0]
InformationStateTensor(0).hand = [0.0, 1.0, 1.0, 0.16667, 0.16667, 1.0, 0.16667, 0.66667, 1.0, 0.33333, 0.83333, 1.0, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0]
InformationStateTensor(0).hand_sizes: ◉◉
InformationStateTensor(0).actions = [0.0, 1.0, 0.0, 1.0, 1.0, 0.16667, 0.16667, 0.16667, 0.16667, 1.0, 0.16667, 0.66667, 0.16667, 0.66667, 1.0, 0.33333, 0.83333, 0.33333, 0.83333, 1.0, 0.66667, 0.66667, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
InformationStateTensor(0).edges: ◯◯◯
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).count_unseen_pips = [4.0, 6.0, 6.0, 5.0, 4.0, 4.0, 7.0]
InformationStateTensor(1).hand = [0.0, 0.5, 1.0, 0.0, 0.66667, 1.0, 0.0, 0.83333, 1.0, 0.16667, 0.83333, 1.0, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 1.0]
InformationStateTensor(1).hand_sizes: ◉◉
InformationStateTensor(1).actions = [0.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.66667, 0.0, 0.66667, 1.0, 0.0, 0.83333, 0.0, 0.83333, 1.0, 0.16667, 0.83333, 0.16667, 0.83333, 1.0, 0.33333, 0.33333, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 0.66667, 0.83333, 1.0]
InformationStateTensor(1).edges: ◯◯◯
ObservationString(0) = "p0 hand:[(0.0, 6.0), (1.0, 1.0), (1.0, 4.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
ObservationString(1) = "p1 hand:[(0.0, 3.0), (0.0, 4.0), (0.0, 5.0), (1.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
PublicObservationString() = "p0 hand:[(0.0, 6.0), (1.0, 1.0), (1.0, 4.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
PrivateObservationString(0) = "p0 hand:[(0.0, 6.0), (1.0, 1.0), (1.0, 4.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
PrivateObservationString(1) = "p1 hand:[(0.0, 3.0), (0.0, 4.0), (0.0, 5.0), (1.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).count_unseen_pips = [6.0, 5.0, 6.0, 7.0, 4.0, 6.0, 4.0]
ObservationTensor(0).hand = [0.0, 1.0, 1.0, 0.16667, 0.16667, 1.0, 0.16667, 0.66667, 1.0, 0.33333, 0.83333, 1.0, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0]
ObservationTensor(0).hand_sizes: ◉◉
ObservationTensor(0).actions = [0.0, 1.0, 0.0, 1.0, 1.0, 0.16667, 0.16667, 0.16667, 0.16667, 1.0, 0.16667, 0.66667, 0.16667, 0.66667, 1.0, 0.33333, 0.83333, 0.33333, 0.83333, 1.0, 0.66667, 0.66667, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
ObservationTensor(0).edges: ◯◯◯
ObservationTensor(1).player: ◯◉
ObservationTensor(1).count_unseen_pips = [4.0, 6.0, 6.0, 5.0, 4.0, 4.0, 7.0]
ObservationTensor(1).hand = [0.0, 0.5, 1.0, 0.0, 0.66667, 1.0, 0.0, 0.83333, 1.0, 0.16667, 0.83333, 1.0, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 1.0]
ObservationTensor(1).hand_sizes: ◉◉
ObservationTensor(1).actions = [0.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.66667, 0.0, 0.66667, 1.0, 0.0, 0.83333, 0.0, 0.83333, 1.0, 0.16667, 0.83333, 0.16667, 0.83333, 1.0, 0.33333, 0.33333, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 0.66667, 0.83333, 1.0]
ObservationTensor(1).edges: ◯◯◯
Rewards() = [0, 0]
Returns() = [0, 0]
LegalActions() = [0, 4, 10, 17, 35, 41, 53]
StringLegalActions() = ["p0 tile:(6.0, 6.0) pip:None", "p0 tile:(4.0, 4.0) pip:None", "p0 tile:(1.0, 1.0) pip:None", "p0 tile:(4.0, 6.0) pip:None", "p0 tile:(2.0, 5.0) pip:None", "p0 tile:(0.0, 6.0) pip:None", "p0 tile:(1.0, 4.0) pip:None"]

# Apply action "p0 tile:(1.0, 1.0) pip:None"
action: 10

# State 15
# hand0:['(0.0, 6.0)', '(1.0, 4.0)', '(2.0, 5.0)', '(4.0, 4.0)', '(4.0, 6.0)', '(6.0, 6.0)'] hand1:['(0.0, 3.0)', '(0.0, 4.0)', '(0.0, 5.0)', '(1.0, 5.0)', '(2.0, 2.0)', '(3.0, 4.0)', '(4.0, 5.0)'] history:['p0 tile:(1.0, 1.0) pip:None']
IsTerminal() = False
History() = [19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2, 10]
HistoryString() = "19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2, 10"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "p0 hand:[(0.0, 6.0), (1.0, 4.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
InformationStateString(1) = "p1 hand:[(0.0, 3.0), (0.0, 4.0), (0.0, 5.0), (1.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).count_unseen_pips = [6.0, 5.0, 6.0, 7.0, 4.0, 6.0, 4.0]
InformationStateTensor(0).hand = [0.0, 1.0, 1.0, 0.16667, 0.66667, 1.0, 0.33333, 0.83333, 1.0, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).hand_sizes = [0.85714, 1.0]
InformationStateTensor(0).actions = [0.16667, 0.66667, 0.66667, 0.16667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).edges = [0.16667, 0.16667, 1.0]
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).count_unseen_pips = [4.0, 5.0, 6.0, 5.0, 4.0, 4.0, 7.0]
InformationStateTensor(1).hand = [0.0, 0.5, 1.0, 0.0, 0.66667, 1.0, 0.0, 0.83333, 1.0, 0.16667, 0.83333, 1.0, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 1.0]
InformationStateTensor(1).hand_sizes = [1.0, 0.85714]
InformationStateTensor(1).actions = [0.16667, 0.83333, 0.83333, 0.16667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).edges = [0.16667, 0.16667, 1.0]
ObservationString(0) = "p0 hand:[(0.0, 6.0), (1.0, 4.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
ObservationString(1) = "p1 hand:[(0.0, 3.0), (0.0, 4.0), (0.0, 5.0), (1.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
PublicObservationString() = "p0 hand:[(0.0, 6.0), (1.0, 4.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
PrivateObservationString(0) = "p0 hand:[(0.0, 6.0), (1.0, 4.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
PrivateObservationString(1) = "p1 hand:[(0.0, 3.0), (0.0, 4.0), (0.0, 5.0), (1.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).count_unseen_pips = [6.0, 5.0, 6.0, 7.0, 4.0, 6.0, 4.0]
ObservationTensor(0).hand = [0.0, 1.0, 1.0, 0.16667, 0.66667, 1.0, 0.33333, 0.83333, 1.0, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]
ObservationTensor(0).hand_sizes = [0.85714, 1.0]
ObservationTensor(0).actions = [0.16667, 0.66667, 0.66667, 0.16667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).edges = [0.16667, 0.16667, 1.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).count_unseen_pips = [4.0, 5.0, 6.0, 5.0, 4.0, 4.0, 7.0]
ObservationTensor(1).hand = [0.0, 0.5, 1.0, 0.0, 0.66667, 1.0, 0.0, 0.83333, 1.0, 0.16667, 0.83333, 1.0, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 1.0]
ObservationTensor(1).hand_sizes = [1.0, 0.85714]
ObservationTensor(1).actions = [0.16667, 0.83333, 0.83333, 0.16667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(1).edges = [0.16667, 0.16667, 1.0]
Rewards() = [0, 0]
Returns() = [0, 0]
LegalActions() = [122]
StringLegalActions() = ["p1 tile:(1.0, 5.0) pip:1.0"]

# Apply action "p1 tile:(1.0, 5.0) pip:1.0"
action: 122

# State 16
# hand0:['(0.0, 6.0)', '(1.0, 4.0)', '(2.0, 5.0)', '(4.0, 4.0)', '(4.0, 6.0)', '(6.0, 6.0)'] hand1:['(0.0, 3.0)', '(0.0, 4.0)', '(0.0, 5.0)', '(2.0, 2.0)', '(3.0, 4.0)', '(4.0, 5.0)'] history:['p0 tile:(1.0, 1.0) pip:None', 'p1 tile:(1.0, 5.0) pip:1.0']
IsTerminal() = False
History() = [19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2, 10, 122]
HistoryString() = "19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2, 10, 122"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "p0 hand:[(0.0, 6.0), (1.0, 4.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
InformationStateString(1) = "p1 hand:[(0.0, 3.0), (0.0, 4.0), (0.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).count_unseen_pips = [6.0, 4.0, 6.0, 7.0, 4.0, 5.0, 4.0]
InformationStateTensor(0).hand = [0.0, 1.0, 1.0, 0.16667, 0.66667, 1.0, 0.33333, 0.83333, 1.0, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).hand_sizes = [0.85714, 0.85714]
InformationStateTensor(0).actions = [0.16667, 0.66667, 0.66667, 0.83333, 1.0, 0.33333, 0.83333, 0.33333, 0.16667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).edges = [0.16667, 0.83333, 1.0]
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).count_unseen_pips = [4.0, 5.0, 6.0, 5.0, 4.0, 4.0, 7.0]
InformationStateTensor(1).hand = [0.0, 0.5, 1.0, 0.0, 0.66667, 1.0, 0.0, 0.83333, 1.0, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 1.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).hand_sizes = [0.85714, 0.85714]
InformationStateTensor(1).actions = [0.0, 0.83333, 0.0, 0.16667, 1.0, 0.66667, 0.83333, 0.66667, 0.16667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).edges = [0.16667, 0.83333, 1.0]
ObservationString(0) = "p0 hand:[(0.0, 6.0), (1.0, 4.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
ObservationString(1) = "p1 hand:[(0.0, 3.0), (0.0, 4.0), (0.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
PublicObservationString() = "p0 hand:[(0.0, 6.0), (1.0, 4.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
PrivateObservationString(0) = "p0 hand:[(0.0, 6.0), (1.0, 4.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
PrivateObservationString(1) = "p1 hand:[(0.0, 3.0), (0.0, 4.0), (0.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).count_unseen_pips = [6.0, 4.0, 6.0, 7.0, 4.0, 5.0, 4.0]
ObservationTensor(0).hand = [0.0, 1.0, 1.0, 0.16667, 0.66667, 1.0, 0.33333, 0.83333, 1.0, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]
ObservationTensor(0).hand_sizes = [0.85714, 0.85714]
ObservationTensor(0).actions = [0.16667, 0.66667, 0.66667, 0.83333, 1.0, 0.33333, 0.83333, 0.33333, 0.16667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).edges = [0.16667, 0.83333, 1.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).count_unseen_pips = [4.0, 5.0, 6.0, 5.0, 4.0, 4.0, 7.0]
ObservationTensor(1).hand = [0.0, 0.5, 1.0, 0.0, 0.66667, 1.0, 0.0, 0.83333, 1.0, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 1.0, 0.0, 0.0, 0.0]
ObservationTensor(1).hand_sizes = [0.85714, 0.85714]
ObservationTensor(1).actions = [0.0, 0.83333, 0.0, 0.16667, 1.0, 0.66667, 0.83333, 0.66667, 0.16667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(1).edges = [0.16667, 0.83333, 1.0]
Rewards() = [0, 0]
Returns() = [0, 0]
LegalActions() = [37, 54]
StringLegalActions() = ["p0 tile:(2.0, 5.0) pip:5.0", "p0 tile:(1.0, 4.0) pip:1.0"]

# Apply action "p0 tile:(1.0, 4.0) pip:1.0"
action: 54

# State 17
# hand0:['(0.0, 6.0)', '(2.0, 5.0)', '(4.0, 4.0)', '(4.0, 6.0)', '(6.0, 6.0)'] hand1:['(0.0, 3.0)', '(0.0, 4.0)', '(0.0, 5.0)', '(2.0, 2.0)', '(3.0, 4.0)', '(4.0, 5.0)'] history:['p0 tile:(1.0, 1.0) pip:None', 'p1 tile:(1.0, 5.0) pip:1.0', 'p0 tile:(1.0, 4.0) pip:1.0']
IsTerminal() = False
History() = [19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2, 10, 122, 54]
HistoryString() = "19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2, 10, 122, 54"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "p0 hand:[(0.0, 6.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
InformationStateString(1) = "p1 hand:[(0.0, 3.0), (0.0, 4.0), (0.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).count_unseen_pips = [6.0, 4.0, 6.0, 7.0, 4.0, 5.0, 4.0]
InformationStateTensor(0).hand = [0.0, 1.0, 1.0, 0.33333, 0.83333, 1.0, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).hand_sizes = [0.71429, 0.85714]
InformationStateTensor(0).actions = [0.33333, 0.83333, 0.33333, 0.66667, 1.0, 0.66667, 0.66667, 0.66667, 0.83333, 1.0, 0.66667, 1.0, 1.0, 0.83333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).edges = [0.66667, 0.83333, 1.0]
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).count_unseen_pips = [4.0, 4.0, 6.0, 5.0, 3.0, 4.0, 7.0]
InformationStateTensor(1).hand = [0.0, 0.5, 1.0, 0.0, 0.66667, 1.0, 0.0, 0.83333, 1.0, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 1.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).hand_sizes = [0.85714, 0.71429]
InformationStateTensor(1).actions = [0.0, 0.66667, 0.0, 0.83333, 1.0, 0.0, 0.83333, 0.0, 0.66667, 1.0, 0.5, 0.66667, 0.5, 0.83333, 1.0, 0.66667, 0.83333, 0.83333, 0.83333, 1.0, 0.66667, 0.83333, 0.66667, 0.66667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).edges = [0.66667, 0.83333, 1.0]
ObservationString(0) = "p0 hand:[(0.0, 6.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
ObservationString(1) = "p1 hand:[(0.0, 3.0), (0.0, 4.0), (0.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
PublicObservationString() = "p0 hand:[(0.0, 6.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
PrivateObservationString(0) = "p0 hand:[(0.0, 6.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
PrivateObservationString(1) = "p1 hand:[(0.0, 3.0), (0.0, 4.0), (0.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).count_unseen_pips = [6.0, 4.0, 6.0, 7.0, 4.0, 5.0, 4.0]
ObservationTensor(0).hand = [0.0, 1.0, 1.0, 0.33333, 0.83333, 1.0, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).hand_sizes = [0.71429, 0.85714]
ObservationTensor(0).actions = [0.33333, 0.83333, 0.33333, 0.66667, 1.0, 0.66667, 0.66667, 0.66667, 0.83333, 1.0, 0.66667, 1.0, 1.0, 0.83333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).edges = [0.66667, 0.83333, 1.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).count_unseen_pips = [4.0, 4.0, 6.0, 5.0, 3.0, 4.0, 7.0]
ObservationTensor(1).hand = [0.0, 0.5, 1.0, 0.0, 0.66667, 1.0, 0.0, 0.83333, 1.0, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 1.0, 0.0, 0.0, 0.0]
ObservationTensor(1).hand_sizes = [0.85714, 0.71429]
ObservationTensor(1).actions = [0.0, 0.66667, 0.0, 0.83333, 1.0, 0.0, 0.83333, 0.0, 0.66667, 1.0, 0.5, 0.66667, 0.5, 0.83333, 1.0, 0.66667, 0.83333, 0.83333, 0.83333, 1.0, 0.66667, 0.83333, 0.66667, 0.66667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(1).edges = [0.66667, 0.83333, 1.0]
Rewards() = [0, 0]
Returns() = [0, 0]
LegalActions() = [101, 102, 117, 129, 138]
StringLegalActions() = ["p1 tile:(4.0, 5.0) pip:4.0", "p1 tile:(4.0, 5.0) pip:5.0", "p1 tile:(3.0, 4.0) pip:4.0", "p1 tile:(0.0, 5.0) pip:5.0", "p1 tile:(0.0, 4.0) pip:4.0"]

# Apply action "p1 tile:(0.0, 4.0) pip:4.0"
action: 138

# State 18
# hand0:['(0.0, 6.0)', '(2.0, 5.0)', '(4.0, 4.0)', '(4.0, 6.0)', '(6.0, 6.0)'] hand1:['(0.0, 3.0)', '(0.0, 5.0)', '(2.0, 2.0)', '(3.0, 4.0)', '(4.0, 5.0)'] history:['p0 tile:(1.0, 1.0) pip:None', 'p1 tile:(1.0, 5.0) pip:1.0', 'p0 tile:(1.0, 4.0) pip:1.0', 'p1 tile:(0.0, 4.0) pip:4.0']
IsTerminal() = False
History() = [19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2, 10, 122, 54, 138]
HistoryString() = "19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2, 10, 122, 54, 138"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 0
InformationStateString(0) = "p0 hand:[(0.0, 6.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
InformationStateString(1) = "p1 hand:[(0.0, 3.0), (0.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).count_unseen_pips = [5.0, 4.0, 6.0, 7.0, 3.0, 5.0, 4.0]
InformationStateTensor(0).hand = [0.0, 1.0, 1.0, 0.33333, 0.83333, 1.0, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).hand_sizes = [0.71429, 0.71429]
InformationStateTensor(0).actions = [0.0, 1.0, 1.0, 0.83333, 1.0, 0.33333, 0.83333, 0.33333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).edges = [0.0, 0.83333, 1.0]
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).count_unseen_pips = [4.0, 4.0, 6.0, 5.0, 3.0, 4.0, 7.0]
InformationStateTensor(1).hand = [0.0, 0.5, 1.0, 0.0, 0.83333, 1.0, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).hand_sizes = [0.71429, 0.71429]
InformationStateTensor(1).actions = [0.0, 0.5, 0.5, 0.83333, 1.0, 0.0, 0.83333, 0.83333, 0.83333, 1.0, 0.0, 0.83333, 0.0, 0.0, 1.0, 0.66667, 0.83333, 0.66667, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).edges = [0.0, 0.83333, 1.0]
ObservationString(0) = "p0 hand:[(0.0, 6.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
ObservationString(1) = "p1 hand:[(0.0, 3.0), (0.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
PublicObservationString() = "p0 hand:[(0.0, 6.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
PrivateObservationString(0) = "p0 hand:[(0.0, 6.0), (2.0, 5.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
PrivateObservationString(1) = "p1 hand:[(0.0, 3.0), (0.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).count_unseen_pips = [5.0, 4.0, 6.0, 7.0, 3.0, 5.0, 4.0]
ObservationTensor(0).hand = [0.0, 1.0, 1.0, 0.33333, 0.83333, 1.0, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).hand_sizes = [0.71429, 0.71429]
ObservationTensor(0).actions = [0.0, 1.0, 1.0, 0.83333, 1.0, 0.33333, 0.83333, 0.33333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).edges = [0.0, 0.83333, 1.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).count_unseen_pips = [4.0, 4.0, 6.0, 5.0, 3.0, 4.0, 7.0]
ObservationTensor(1).hand = [0.0, 0.5, 1.0, 0.0, 0.83333, 1.0, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(1).hand_sizes = [0.71429, 0.71429]
ObservationTensor(1).actions = [0.0, 0.5, 0.5, 0.83333, 1.0, 0.0, 0.83333, 0.83333, 0.83333, 1.0, 0.0, 0.83333, 0.0, 0.0, 1.0, 0.66667, 0.83333, 0.66667, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(1).edges = [0.0, 0.83333, 1.0]
Rewards() = [0, 0]
Returns() = [0, 0]
LegalActions() = [37, 42]
StringLegalActions() = ["p0 tile:(2.0, 5.0) pip:5.0", "p0 tile:(0.0, 6.0) pip:0.0"]

# Apply action "p0 tile:(2.0, 5.0) pip:5.0"
action: 37

# State 19
# hand0:['(0.0, 6.0)', '(4.0, 4.0)', '(4.0, 6.0)', '(6.0, 6.0)'] hand1:['(0.0, 3.0)', '(0.0, 5.0)', '(2.0, 2.0)', '(3.0, 4.0)', '(4.0, 5.0)'] history:['p0 tile:(1.0, 1.0) pip:None', 'p1 tile:(1.0, 5.0) pip:1.0', 'p0 tile:(1.0, 4.0) pip:1.0', 'p1 tile:(0.0, 4.0) pip:4.0', 'p0 tile:(2.0, 5.0) pip:5.0']
IsTerminal() = False
History() = [19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2, 10, 122, 54, 138, 37]
HistoryString() = "19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2, 10, 122, 54, 138, 37"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = 1
InformationStateString(0) = "p0 hand:[(0.0, 6.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
InformationStateString(1) = "p1 hand:[(0.0, 3.0), (0.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).count_unseen_pips = [5.0, 4.0, 6.0, 7.0, 3.0, 5.0, 4.0]
InformationStateTensor(0).hand = [0.0, 1.0, 1.0, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).hand_sizes = [0.57143, 0.71429]
InformationStateTensor(0).actions = [0.0, 1.0, 1.0, 0.33333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).edges = [0.0, 0.33333, 1.0]
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).count_unseen_pips = [4.0, 4.0, 5.0, 5.0, 3.0, 3.0, 7.0]
InformationStateTensor(1).hand = [0.0, 0.5, 1.0, 0.0, 0.83333, 1.0, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).hand_sizes = [0.71429, 0.57143]
InformationStateTensor(1).actions = [0.0, 0.5, 0.5, 0.33333, 1.0, 0.0, 0.83333, 0.83333, 0.33333, 1.0, 0.33333, 0.33333, 0.33333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(1).edges = [0.0, 0.33333, 1.0]
ObservationString(0) = "p0 hand:[(0.0, 6.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
ObservationString(1) = "p1 hand:[(0.0, 3.0), (0.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
PublicObservationString() = "p0 hand:[(0.0, 6.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
PrivateObservationString(0) = "p0 hand:[(0.0, 6.0), (4.0, 4.0), (4.0, 6.0), (6.0, 6.0)]"
PrivateObservationString(1) = "p1 hand:[(0.0, 3.0), (0.0, 5.0), (2.0, 2.0), (3.0, 4.0), (4.0, 5.0)]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).count_unseen_pips = [5.0, 4.0, 6.0, 7.0, 3.0, 5.0, 4.0]
ObservationTensor(0).hand = [0.0, 1.0, 1.0, 0.66667, 0.66667, 1.0, 0.66667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).hand_sizes = [0.57143, 0.71429]
ObservationTensor(0).actions = [0.0, 1.0, 1.0, 0.33333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).edges = [0.0, 0.33333, 1.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).count_unseen_pips = [4.0, 4.0, 5.0, 5.0, 3.0, 3.0, 7.0]
ObservationTensor(1).hand = [0.0, 0.5, 1.0, 0.0, 0.83333, 1.0, 0.33333, 0.33333, 1.0, 0.5, 0.66667, 1.0, 0.66667, 0.83333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(1).hand_sizes = [0.71429, 0.57143]
ObservationTensor(1).actions = [0.0, 0.5, 0.5, 0.33333, 1.0, 0.0, 0.83333, 0.83333, 0.33333, 1.0, 0.33333, 0.33333, 0.33333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(1).edges = [0.0, 0.33333, 1.0]
Rewards() = [0, 0]
Returns() = [0, 0]
LegalActions() = [86, 128, 143]
StringLegalActions() = ["p1 tile:(2.0, 2.0) pip:2.0", "p1 tile:(0.0, 5.0) pip:0.0", "p1 tile:(0.0, 3.0) pip:0.0"]

# Apply action "p1 tile:(0.0, 5.0) pip:0.0"
action: 128

# State 20
# Apply action "p1 tile:(2.0, 2.0) pip:2.0"
action: 86

# State 21
# Apply action "p1 tile:(4.0, 5.0) pip:5.0"
action: 102

# State 22
# Apply action "p0 tile:(4.0, 6.0) pip:4.0"
action: 18

# State 23
# Apply action "p0 tile:(6.0, 6.0) pip:6.0"
action: 1

# State 24
# Apply action "p0 tile:(0.0, 6.0) pip:6.0"
action: 43

# State 25
# Apply action "p1 tile:(0.0, 3.0) pip:0.0"
action: 143

# State 26
# Apply action "p1 tile:(3.0, 4.0) pip:3.0"
action: 116

# State 27
# hand0:['(4.0, 4.0)'] hand1:[] history:['p0 tile:(1.0, 1.0) pip:None', 'p1 tile:(1.0, 5.0) pip:1.0', 'p0 tile:(1.0, 4.0) pip:1.0', 'p1 tile:(0.0, 4.0) pip:4.0', 'p0 tile:(2.0, 5.0) pip:5.0', 'p1 tile:(0.0, 5.0) pip:0.0', 'p1 tile:(2.0, 2.0) pip:2.0', 'p1 tile:(4.0, 5.0) pip:5.0', 'p0 tile:(4.0, 6.0) pip:4.0', 'p0 tile:(6.0, 6.0) pip:6.0', 'p0 tile:(0.0, 6.0) pip:6.0', 'p1 tile:(0.0, 3.0) pip:0.0', 'p1 tile:(3.0, 4.0) pip:3.0']
IsTerminal() = True
History() = [19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2, 10, 122, 54, 138, 37, 128, 86, 102, 18, 1, 43, 143, 116]
HistoryString() = "19, 22, 10, 17, 15, 24, 4, 20, 14, 0, 5, 16, 8, 2, 10, 122, 54, 138, 37, 128, 86, 102, 18, 1, 43, 143, 116"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.TERMINAL
InformationStateString(0) = "p0 hand:[(4.0, 4.0)]"
InformationStateString(1) = "p1 hand:[]"
InformationStateTensor(0).player: ◉◯
InformationStateTensor(0).count_unseen_pips = [3.0, 4.0, 5.0, 5.0, 1.0, 3.0, 4.0]
InformationStateTensor(0).hand = [0.66667, 0.66667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).hand_sizes = [0.14286, 0.0]
InformationStateTensor(0).actions = [0.66667, 0.66667, 0.66667, 0.33333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
InformationStateTensor(0).edges = [0.33333, 0.66667, 1.0]
InformationStateTensor(1).player: ◯◉
InformationStateTensor(1).count_unseen_pips = [3.0, 4.0, 5.0, 5.0, 2.0, 3.0, 4.0]
InformationStateTensor(1).hand: ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
                                ◯◯◯
InformationStateTensor(1).hand_sizes = [0.0, 0.14286]
InformationStateTensor(1).actions: ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
                                   ◯◯◯◯◯
InformationStateTensor(1).edges = [0.33333, 0.66667, 1.0]
ObservationString(0) = "p0 hand:[(4.0, 4.0)]"
ObservationString(1) = "p1 hand:[]"
PublicObservationString() = "p0 hand:[(4.0, 4.0)]"
PrivateObservationString(0) = "p0 hand:[(4.0, 4.0)]"
PrivateObservationString(1) = "p1 hand:[]"
ObservationTensor(0).player: ◉◯
ObservationTensor(0).count_unseen_pips = [3.0, 4.0, 5.0, 5.0, 1.0, 3.0, 4.0]
ObservationTensor(0).hand = [0.66667, 0.66667, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).hand_sizes = [0.14286, 0.0]
ObservationTensor(0).actions = [0.66667, 0.66667, 0.66667, 0.33333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
ObservationTensor(0).edges = [0.33333, 0.66667, 1.0]
ObservationTensor(1).player: ◯◉
ObservationTensor(1).count_unseen_pips = [3.0, 4.0, 5.0, 5.0, 2.0, 3.0, 4.0]
ObservationTensor(1).hand: ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
                           ◯◯◯
ObservationTensor(1).hand_sizes = [0.0, 0.14286]
ObservationTensor(1).actions: ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
                              ◯◯◯◯◯
ObservationTensor(1).edges = [0.33333, 0.66667, 1.0]
Rewards() = [-18, 18]
Returns() = [-18, 18]
